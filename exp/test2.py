from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import torch
import os

# è¼‰å…¥æ¨¡å‹èˆ‡è™•ç†å™¨
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

# æ‰€æœ‰ 6 å€‹è³‡æ–™å¤¾è·¯å¾‘
image_folders = [
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_1_images/",
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_2_images/",
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_3_images/",
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_4_images/",
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_5_images/",
    "./output_images/1Q25_MP_Chinese_Vuploadbatch_6_images/",
]

# æœ€çµ‚è¦å•çš„å•é¡Œ
final_question = "æ–‡ä»¶ä¸­ã€Œæ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šå‡ºç¾åœ¨ç°¡å ±çš„å“ªä¸€é ï¼Ÿ"

# === åˆ†æ‰¹å»ºç«‹å°è©± ===
dialog = []

for i, folder in enumerate(image_folders):
    image_files = sorted([
        os.path.join(folder, f)
        for f in os.listdir(folder)
        if f.endswith(('.png', '.jpg', '.jpeg'))
    ])[:10]  # æ¯æ‰¹é™åˆ¶åœ–ç‰‡æ•¸ï¼ˆå¯è©¦ 2~4ï¼‰

    # è½‰ç‚º PIL ä¸¦ resize
    images = [Image.open(p).convert("RGB").resize((720, 720)) for p in image_files]

    # åŠ å…¥ä¸€è¼ªè¨Šæ¯ï¼ˆæ¨¡æ“¬è®“ VLM è¨˜ä½é€™æ‰¹åœ–ï¼‰
    message_content = [{"type": "image", "image": img} for img in images]
    message_content.append({"type": "text", "text": f"é€™æ˜¯ç¬¬ {i+1} æ‰¹åœ–è¡¨è³‡æ–™ï¼Œè«‹è¨˜ä½å®ƒå€‘ã€‚"})
    
    dialog.append({
        "role": "user",
        "content": message_content
    })

# æœ€å¾ŒåŠ å…¥çœŸæ­£çš„å•é¡Œ
dialog.append({
    "role": "user",
    "content": [{"type": "text", "text": final_question}]
})

try:
    # ç·¨ç¢¼è™•ç†
    text = processor.apply_chat_template(dialog, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(dialog)

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to("cuda")

    # æ¨ç†
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=400)

    # è§£ç¢¼
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    # è¼¸å‡º
    print("\nğŸ“Š å•é¡Œï¼š", final_question)
    print("ğŸ§  å›ç­”ï¼š", output_text.strip())
    print("=" * 100)

except torch.cuda.OutOfMemoryError:
    print("âŒ CUDA è¨˜æ†¶é«”ä¸è¶³ï¼Œè«‹é™ä½æ¯æ‰¹åœ–ç‰‡æ•¸æˆ–ç¸®åœ–å°ºå¯¸")
    torch.cuda.empty_cache()
