import torch
import os
import csv
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from rouge_score import rouge_scorer
from dotenv import load_dotenv
load_dotenv()
import re
import unicodedata
import openai
import base64
import io
import argparse


# def openai_inference(image_path, model_key, OPENAI_API_KEY ,question_key ,ground_text=None, MAX_NEW_TOKEN=128, TEMPERATURE=0, repetition_penalty=1.05):
#     client = OpenAI(api_key=OPENAI_API_KEY)
#     model = AVAILABLE_MODELS.get(model_key, AVAILABLE_MODELS[model_key])
#     image_base64 = encode_image(image_path)
#     question = generate_text(question_key, ground_text)[0]
#     response = client.chat.completions.create(
#                 model=model,
#                 messages=[
#                     {
#                     "role": "user",
#                     "content": [
#                                     {"type": "text", "text": question},
#                                     {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
#                                 ],
#                     }
#                 ],
#                 max_tokens=MAX_NEW_TOKEN,
#                 temperature=TEMPERATURE,
#                 frequency_penalty=repetition_penalty,
#                 )
#     output_text = [response.choices[0].message.content]
#     prediction = process_and_clean_text(output_text)
#     return prediction

# ===== åˆå§‹åŒ–æ¨¡å‹ =====
model = AutoModel.from_pretrained(
    'openbmb/MiniCPM-V-2_6',
    trust_remote_code=True,
    attn_implementation='sdpa',
    torch_dtype=torch.bfloat16
).eval().cuda()

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)

# ===== è¼‰å…¥åœ–ç‰‡æ‰¹æ¬¡ =====
def load_image_batches(folder_path, batch_size):
    image_files = sorted([
        os.path.join(folder_path, f)
        for f in os.listdir(folder_path)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ])
    for i in range(0, len(image_files), batch_size):
        paths = image_files[i:i + batch_size]
        images = [Image.open(p).convert("RGB").resize((720, 720)) for p in paths]
        yield images

Prompt = {"Locate": "è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
          "è«‹ç°¡ç­”ï¼Œä¾‹å¦‚ï¼šã€Œxxxåœ¨ç¬¬å¹¾é ï¼Ÿã€æˆ–ã€Œxxxåœ¨å“ªè£¡ã€è«‹å›ç­” ã€Œç¬¬yé ã€æˆ–æœ‰è¶…éä¸€é å¯ä»¥å›ç­”ã€Œç¬¬yé å’Œã€ç¬¬zé ã€"
          "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œ",
          
          "Compare": "è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
          "è«‹ç°¡ç­”ï¼Œä¾‹å¦‚:å•é¡Œå•æ•¸å­—ç›¸é—œçš„å•é¡Œä¸¦ä¸”æœ‰å–®ä½ï¼Œè«‹ç›´æ¥å›ç­”æ•¸å­—å³å¯ã€‚è‹¥ç„¡å–®ä½è«‹å‹™å¿…åŠ ä¸Šå–®ä½ã€‚"
          "ç¹é«”ä¸­æ–‡å›ç­”",
          
          "Rank":"è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
          "å¦‚æœé‡åˆ°æœ€é«˜æˆ–æ˜¯æœ€ä½è«‹ç›´æ¥ä»¥æ–‡å­—å›ç­”æœ€é«˜æˆ–æ˜¯æœ€ä½çš„é …ç›®ï¼Œå¦‚æœæ˜¯è«‹ä»¥é«˜ä½æ’åºè«‹ä»¥å¤§æ–¼ï¼ˆ>ï¼‰æˆ–æ˜¯å°æ–¼ï¼ˆ<ï¼‰ç¬¦è™Ÿè¡¨ç¤ºã€‚ä¾‹å¦‚ï¼šã€ŒA>B>Cã€ï¼Œåä¹‹äº¦ç„¶ã€‚"
          "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œ",

          "Trend": "è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
          "å•è¶¨å‹¢æˆ–æ˜¯è®ŠåŒ–æ™‚å¯ä»¥ä¸Šå‡ä¸‹é™æˆ–æ˜¯æŒå¹³ç‚ºç­”æ¡ˆï¼Œä½†æœ‰æ™‚æœƒå‡ºç¾å…ˆå‡å¾Œé™æˆ–æ˜¯å…ˆå°‡å¾Œå‡ï¼Œè«‹ä»¥å¯¦éš›æƒ…æ³ç‚ºä¸»ã€‚"
          "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚",
          
          "Extract":"è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
          "è«‹ä»¥æœ€é–“å–®çš„æ–¹å¼å›ç­”ï¼Œå¦‚éœ€åˆ—å‡ºå¤šå€‹é …ç›®ï¼Œè‹¥ç‚ºä¸­æ–‡è«‹ä»¥é “è™Ÿï¼ˆã€ï¼‰åˆ†éš”ï¼Œè‹¥ç‚ºè‹±æ–‡è«‹ä»¥é€—è™Ÿï¼ˆ,ï¼‰åˆ†éš”ã€‚"
          "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"}

# ===== å–®æ¬¡æå•ï¼ˆé™„åŠ å¤±æ•—æç¤ºï¼‰=====
def ask(images, question , prompt_type="Locate"):
    prompt = Prompt[prompt_type]
    full_question = f"{question}\n\n{prompt}"
    msgs = [{'role': 'user', 'content': images + [full_question]}]
    with torch.no_grad():
        answer = model.chat(
            image=None, 
            msgs=msgs, 
            tokenizer=tokenizer,
            sampling=True,             # ä½¿å›ç­”å¯é‡ç¾ï¼Œé¿å…èƒ¡äº‚ç”Ÿæˆ
            temperature=0.2,            # è¶¨è¿‘ deterministicï¼Œé©åˆ QA è©•ä¼°ä»»å‹™
            max_new_tokens=256)          # å›ç­”ä¸å¤ å®Œæ•´æ™‚å¯èª¿å¤§ï¼ˆå¦‚ 512ï¼‰)
    return answer

# ===== æ•´åˆæ‘˜è¦å†æ¬¡æå• =====
def summarize_all(summaries, question):
    combined = "\n".join([f"æ‰¹æ¬¡{i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    final_prompt = f"ä»¥ä¸‹æ˜¯æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦ï¼š\n{combined}\n\nè«‹æ ¹æ“šä»¥ä¸‹æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦å…§å®¹ï¼Œæ‰¾å‡ºæœ€åˆç†çš„å…·é«”ç­”æ¡ˆï¼Œåªéœ€ç›´æ¥å›ç­”çµæœå³å¯ï¼Œæ•¬è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š{question}"
    return ask([], final_prompt)

# ===== è©•ä¼°æŒ‡æ¨™ =====
def substring_hit(pred, ref):
    pred = pred.lower().strip()
    ref = ref.lower().strip()
    ref_keywords = ref.split()
    return int(any(word in pred for word in ref_keywords))


def evaluate_answer(pred, ref):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_l_score = scorer.score(ref, pred)['rougeL'].fmeasure
    hit_score = substring_hit(pred, ref)
    return {
        "rougeL_f1": round(rouge_l_score, 4),
        "substring_hit": hit_score,
    }

# ===== è™•ç† CSV å•é¡Œ =====
def process_csv_questions(csv_path, image_folder, batch_size):
    results = []

    # é å…ˆè¼‰å…¥æ‰€æœ‰åœ–ç‰‡æ‰¹æ¬¡
    image_batches = list(load_image_batches(image_folder, batch_size=batch_size))

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, 1):
            question = row['Question']
            reference = row['Short Answer']
            print(f"\n============= å•é¡Œ {idx}: {question} =============")

            summaries = []
            for i, images in enumerate(image_batches, 1):
                try:
                    summary = ask(images, question)
                except Exception as e:
                    print(f"  âŒ åœ–ç‰‡æ‰¹æ¬¡ {i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    summary = "(ERROR)"
                summaries.append(summary)

            final_answer = summarize_all(summaries, question)
            print(f"âœ… æœ€çµ‚å›ç­”ï¼š{final_answer}")

            metrics = evaluate_answer(final_answer, reference)
            results.append({
                "id": idx,
                "question": question,
                "prediction": final_answer,
                "reference": reference,
                **metrics
            })

    return results

# ===== ä¸»æµç¨‹ =====
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv_path", type=str, default='./QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - è³‡æ–™æå–å•é¡Œ.csv')
    parser.add_argument("--folder", type=str, default='./output_images_fitz')
    parser.add_argument("--batch_size", type=int, default=2)
    args = parser.parse_args()

    # csv_path = './QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - è³‡æ–™æå–å•é¡Œ.csv'
    # folder = './output_images_fitz'
    csv_path = args.csv_path
    folder = args.folder
    batch_size = args.batch_size


    results = process_csv_questions(csv_path, folder, batch_size=batch_size)

    # çµ±è¨ˆ
    total_rouge = 0
    total_hit = 0
    count = len(results)

    print("\nğŸ“‹ æ¯ä¸€é¡Œè©•ä¼°çµæœï¼š")
    for r in results:
        rouge = r.get('rougeL_f1', 0)
        hit = r.get('substring_hit', 0)
        print(f"ID {r['id']} | ROUGE-L F1: {rouge} | Substring Hit: {hit}")

        total_rouge += rouge
        total_hit += hit

    avg_rouge = total_rouge / count if count > 0 else 0
    avg_hit = total_hit / count if count > 0 else 0

    print("\nğŸ“Š Evaluation Summary:")
    print(f"ğŸ”¹ Average ROUGE-L F1: {round(avg_rouge, 4)}")
    print(f"ğŸ”¹ Average Substring Hit Rate: {round(avg_hit, 4)}")
