import torch
import os
import csv
from PIL import Image
from rouge_score import rouge_scorer
from dotenv import load_dotenv
load_dotenv()
import re
import unicodedata
import openai
import base64
import io
import time
import pandas as pd
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import json

# ===== åˆå§‹åŒ–æ¨¡å‹ =====
model_name_or_path = "Qwen/Qwen2.5-VL-7B-Instruct"

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_name_or_path,
    torch_dtype="auto",
    device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_name_or_path)

# ===== Prompt å®šç¾© =====
Prompt = {
    "Locate": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚
    ä¾‹å¦‚ï¼šã€Œæ–‡ä»¶ä¸­ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç°¡å ±çš„å“ªä¸€é ï¼Ÿã€
    è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š
    {
      "answer": "è©³ç­”å…§å®¹ï¼ˆä¾‹å¦‚ï¼šã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é ï¼‰",
      "page": [é ç¢¼æ•¸å­—]
    }
    å¦‚æœåœ–è¡¨å‡ºç¾åœ¨å¤šé ï¼Œè«‹ç”¨æ•´æ•¸é™£åˆ—æ¨™ç¤ºæ‰€æœ‰é ç¢¼ï¼Œä¾‹å¦‚ï¼š
    {
      "answer": "ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é å’Œç¬¬12é ",
      "page": [5, 12]
    }
    å¦‚æœç„¡æ³•æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ï¼š
    {
      "answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ã€æ¯è‚¡ç›ˆé¤˜ã€çš„è³‡è¨Š",
      "page": []
    }''',
    "Compare": '''â€¦ï¼ˆç•¥ï¼ŒåŒä¸Šï¼Œä¿ç•™åŸ prompt å…§å®¹ï¼‰â€¦''',
    "Rank":   '''â€¦''',
    "Trend":  '''â€¦''',
    "Extract":'''â€¦''',
    "Relevance": '''åˆ¤æ–·é€™å¼µåœ–ç‰‡çš„å…§å®¹æ˜¯å¦èˆ‡ä»¥ä¸‹å•é¡Œæœ‰é—œï¼š
    ã€Œ{question}ã€
    è«‹å‹™å¿…åªè¼¸å‡ºä¸€å€‹å­—ï¼š
    - å¦‚æœæœ‰é—œï¼Œè«‹å›ç­”ï¼šæ˜¯
    - å¦‚æœç„¡é—œæˆ–ç„¡æ³•åˆ¤æ–·ï¼Œè«‹å›ç­”ï¼šå¦
    '''
}
bginfo = "å¤§æ¨™é¡Œé€šå˜—åœ¨å·¦ä¸Šè§’ï¼Œé ç¢¼ä½æ–¼å³ä¸‹è§’ã€‚é•·æ¢åœ–çš„æ¨™é¡Œä½æ–¼åœ–ç‰‡æ­£ä¸Šæ–¹ã€‚"

# ===== VLM æ¨è«–å‡½å¼ =====
def vlm_answer(image: Image.Image, question: str, prompt_type: str) -> str:
    if prompt_type == "Relevance":
        prompt = Prompt["Relevance"].format(question=question)
        max_tokens = 5
    else:
        prompt = Prompt[prompt_type] + f"\n\n{bginfo}\n\n"
        prompt = f"{question}\n\n{prompt}"
        max_tokens = 256

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt}
        ]
    }]
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)
    gen_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    output = processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()

    # æ¸…æ‰å¤šé¤˜æ¨¡æ¿ï¼Œåªä¿ç•™æœ€å¾Œä¸€è¡Œ
    output = output.split("\n")[-1].strip()

    # å¦‚æœæ˜¯å–®å€‹ } æˆ– { æˆ–ç©ºå­—ä¸²ï¼Œè¦–ç‚ºç„¡æ•ˆ
    if output in ["}", "{", ""]:
        return '{"short_answer": "ç„¡æ³•å–å¾—", "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š", "page": []}'

    return output

# ===== ç¯©é¸ç›¸é—œé é¢ =====
def filter_related_pages_by_vlm(image_folder, question, vlm_answer_fn, resize=(720, 720)):
    related_pages = []
    for fn in sorted(os.listdir(image_folder)):
        if fn.lower().endswith(".png"):
            path = os.path.join(image_folder, fn)
            img = Image.open(path).resize(resize)
            match = re.search(r'\d+', fn)
            page_num = int(match.group()) if match else fn

            answer = vlm_answer_fn(img, question, "Relevance").strip()
            print(f"ğŸ§ é  {page_num} â†’ å›ç­”ï¼šã€Œ{answer}ã€")

            if "æ˜¯" in answer and "å¦" not in answer:
                related_pages.append({"page": page_num, "image_path": path})

    return related_pages

# ===== åŸ·è¡Œ VLM å•å®Œæ•´å•é¡Œ =====
def run_vlm(matched_pages, question, vlm_answer_fn, prompt_type, resize=(720, 720)):
    results = []
    for info in matched_pages:
        img = Image.open(info['image_path']).resize(resize)
        answer = vlm_answer_fn(img, question, prompt_type)
        print(f"ğŸ“„ ç¬¬ {info['page']} é  â†’ åŸå§‹è¼¸å‡ºï¼š{repr(answer)}")
        results.append({"page": info['page'], "answer": answer})
    return results

# ===== æ•´ç†æœ€çµ‚ç­”æ¡ˆï¼ˆå·²éæ¿¾ç„¡æ•ˆå›ç­”ï¼‰ =====
def summarize_answers(results):
    # éæ¿¾æ‰æ‰€æœ‰ã€Œç„¡æ³•å–å¾—ã€
    valid_results = [r for r in results if '"ç„¡æ³•å–å¾—"' not in r["answer"]]
    if not valid_results:
        return {"final_answer": "æ‰¾ä¸åˆ°ä»»ä½•ç›¸é—œè³‡è¨Š", "pages": []}
    # é¸æœ€é•·çš„ç­”æ¡ˆ
    best = max(valid_results, key=lambda r: len(r["answer"]))
    return {
        "final_answer": best["answer"],
        "pages": [r["page"] for r in valid_results]
    }

# ===== CSV å·¥å…· =====
def read_column_as_list(csv_path: str, column_name: str) -> list:
    vals = []
    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        if column_name not in reader.fieldnames:
            raise ValueError(f"âŒ æ‰¾ä¸åˆ°æ¬„ä½ï¼š{column_name}")
        for row in reader:
            v = row[column_name].strip()
            if v:
                vals.append(v)
    return vals

# ===== è©•ä¼°å·¥å…· =====
def exact_match_score(pred: str, ref: str) -> int:
    def normalize(txt):
        txt = unicodedata.normalize("NFKC", txt)
        return re.sub(r"\s+", "", txt.strip())
    return int(normalize(pred) == normalize(ref))

def llm_judge_score(question, reference, prediction, model_name, openai_api_key) -> int:
    openai.api_key = openai_api_key
    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI è©•ä¼°åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å…§å®¹çµ¦å‡º 1ï½5 åˆ†ã€‚
---
å•é¡Œï¼š
{question}
æ¨™æº–ç­”æ¡ˆï¼š
{reference}
æ¨¡å‹ç­”æ¡ˆï¼š
{prediction}
åªå›ä¸€å€‹æ•´æ•¸ï¼ˆ1â€“5ï¼‰ã€‚
"""
    try:
        resp = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=10
        )
        return int(re.search(r"[1-5]", resp.choices[0].message.content).group())
    except Exception as e:
        print(f"âš ï¸ è©•åˆ†éŒ¯èª¤ï¼š{e}")
        return -1

def safe_json_load(raw):
    try:
        m = re.search(r"\{.*\}", raw, re.DOTALL)
        return json.loads(m.group()) if m else None
    except Exception as e:
        print(f"âš ï¸ JSON è§£æéŒ¯èª¤ï¼š{e}")
        return None

def evaluate_with_llm(csv_path, model_results, model_name, openai_api_key, output_csv="final_eval_output.csv"):
    df = pd.read_csv(csv_path)
    recs = []
    for itm in model_results:
        q = itm["question"]
        raw = itm["answer"]
        js = safe_json_load(raw)
        short_pred = js.get("short_answer", raw) if js else raw
        long_pred  = js.get("long_answer", raw)  if js else raw

        row = df[df["Question"] == q]
        if row.empty:
            print(f"âŒ æ‰¾ä¸åˆ°é¡Œç›®ï¼š{q}")
            continue
        sr = row.iloc[0]
        em = exact_match_score(short_pred, sr["Short Answer"])
        lj = llm_judge_score(q, sr["Long Answer"], long_pred, model_name, openai_api_key)
        recs.append({
            "Type": sr.get("Chart Type",""),
            "Question": q,
            "Short Answer": sr["Short Answer"],
            "Long Answer": sr["Long Answer"],
            "Prediction": long_pred,
            "Exact Match Score": em,
            "LLM Judge Score": lj,
            "Model": model_name
        })

    final_df = pd.DataFrame(recs)
    final_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… è©•ä¼°å®Œæˆï¼Œå·²è¼¸å‡ºï¼š{output_csv}")
    return final_df

# ===== ä¸»ç¨‹å¼ =====
if __name__ == '__main__':
    image_folder = "./data/pdf_img/1Q25_MP_Chinese_Vupload/"
    csv_path     = "./QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - å®šä½å•é¡Œ.csv"
    questions    = read_column_as_list(csv_path, "Question")
    all_results  = []

    for idx, q in enumerate(questions, 1):
        print(f"\nğŸŸ¢ å•é¡Œ {idx}/{len(questions)}ï¼š{q}")
        # 1. ç¯©é¸ç›¸é—œé 
        pages = filter_related_pages_by_vlm(image_folder, q, vlm_answer)
        # 2. æ­£å¼å›ç­”
        res = run_vlm(pages, q, vlm_answer, prompt_type="Locate")
        final = summarize_answers(res)
        print("ğŸ¯ æœ€çµ‚ç­”æ¡ˆï¼š", final)
        all_results.append({"question": q, "answer": final["final_answer"]})

    # 3. è©•ä¼°
    evaluate_with_llm(
        csv_path, all_results,
        model_name="Qwen2.5-VL-7B-Instruct",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        output_csv="qwen_vlm_eval_with_llm_Locate.csv"
    )
