import torch
import os
import csv
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from rouge_score import rouge_scorer
from dotenv import load_dotenv
load_dotenv()
import re
import unicodedata
import openai
import base64
import io
import time
import pandas as pd

# ===== ğŸ“˜ Function èªªæ˜ç¸½è¦½ =====

# ğŸ”§ å·¥å…·èˆ‡ç·¨ç¢¼
# - stitch_imagesï¼šå°‡å¤šå¼µåœ–ç‰‡æ©«å‘æ‹¼æ¥ç‚ºä¸€å¼µï¼Œç”¨æ–¼æ¨¡å‹æ¨è«–ã€‚
# - encode_imageï¼šå°‡åœ–ç‰‡è½‰æ›ç‚º base64 å­—ä¸²ï¼Œä¾› GPT-4o API ä½¿ç”¨ã€‚

# ğŸ§  GPT-4o æ¨è«–
# - openai_inferenceï¼šå°‡å–®å¼µåœ–ç‰‡èˆ‡å•é¡Œé€å…¥ GPT-4oï¼Œå›å‚³æ–‡å­—ç­”æ¡ˆã€‚

# ğŸ–¼ï¸ VLM æ‰¹æ¬¡æ¨è«–
# - run_vlm_on_all_imagesï¼šå°‡è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰åœ–ç‰‡é€å¼µé€å…¥ VLM ä¸¦å–å¾—å›ç­”ã€‚

# ğŸ§  MiniCPM æ¨è«–é‚è¼¯
# - vlm_answerï¼šå°‡å–®å¼µåœ–ç‰‡èˆ‡ç‰¹å®š prompt é€å…¥ MiniCPM-Vï¼Œå–å¾—å›ç­”ã€‚

# ğŸ” ç¯©é¸ç›¸é—œåœ–ç‰‡é é¢
# - filter_related_pages_by_vlmï¼šé€é è©¢å• VLMã€Œé€™ä¸€é æ˜¯å¦èˆ‡æ­¤å•é¡Œæœ‰é—œã€ï¼Œå›å‚³ç›¸é—œé é¢ã€‚

# â“ å°ç›¸é—œé é¢å•å®Œæ•´é¡Œç›®
# - run_vlmï¼šå°ç¯©é¸å‡ºä¾†çš„ç›¸é—œåœ–ç‰‡é é¢é€²è¡Œå®Œæ•´å•é¡Œå›ç­”ï¼Œå›å‚³æ¯é çš„ç­”æ¡ˆã€‚

# ğŸ“š æ•´ç†çµæœ
# - summarize_answersï¼šå¾æ‰€æœ‰ç­”æ¡ˆä¸­é¸å‡ºæœ€é•·å›ç­”ç•¶ä½œä»£è¡¨ç­”æ¡ˆï¼Œä¸¦æ•´ç†é ç¢¼ã€‚

# ğŸ“„ CSV è³‡æ–™è™•ç†
# - read_column_as_listï¼šå¾ CSV ä¸­æŒ‡å®šæ¬„ä½è®€å–æˆ listï¼Œä¾‹å¦‚å–å¾—æ‰€æœ‰å•é¡Œã€‚

# âœ… è©•ä¼°ç›¸é—œ
# - exact_match_scoreï¼šæª¢æŸ¥é æ¸¬çŸ­ç­”æ¡ˆæ˜¯å¦èˆ‡æ¨™æº–ç­”æ¡ˆå®Œå…¨ä¸€è‡´ï¼ˆå¿½ç•¥æ ¼å¼å·®ç•°ï¼‰ã€‚
# - llm_judge_scoreï¼šå°‡é•·ç­”æ¡ˆäº¤çµ¦ GPT-4o è©•åˆ†èªæ„ç›¸ä¼¼åº¦ï¼ˆ1~5 åˆ†ï¼‰ã€‚
# - evaluate_with_llmï¼šæ•´åˆæ¨¡å‹çµæœèˆ‡è©•åˆ†é‚è¼¯ï¼Œè¼¸å‡ºå®Œæ•´è©•ä¼°çµæœç‚º CSVã€‚

# ğŸ“Œ è£œå……ï¼š
# - bginfoï¼šè¼”åŠ© VLM ç†è§£åœ–ç‰‡æ ¼å¼çš„æç¤ºæ–‡å­—ã€‚
# - Promptï¼šæ ¹æ“šä¸åŒå•é¡Œé¡å‹æä¾›çš„æ¨™æº–åŒ– prompt æ¨¡æ¿ã€‚



# ===== OpenAI gpt-4o api =====
def stitch_images(image_paths):
    images = [Image.open(p).convert("RGB").resize((720, 720)) for p in image_paths]
    total_width = sum(img.width for img in images)
    max_height = max(img.height for img in images)
    stitched = Image.new("RGB", (total_width, max_height))
    x_offset = 0
    for img in images:
        stitched.paste(img, (x_offset, 0))
        x_offset += img.width
    return stitched


def encode_image(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode("utf-8")

def openai_inference(image_obj, question, OPENAI_API_KEY, MAX_NEW_TOKEN=256, TEMPERATURE=0.2, repetition_penalty=1.05):
    openai.api_key = OPENAI_API_KEY
    buffer = io.BytesIO()
    image_obj.save(buffer, format="JPEG")
    image_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ]
            }
        ],
        max_tokens=MAX_NEW_TOKEN,
        temperature=TEMPERATURE,
        frequency_penalty=repetition_penalty
    )

    return response.choices[0].message.content.strip()



def run_vlm_on_all_images(image_folder, question, vlm_answer_fn, prompt_type, bginfo=None, resize=(720, 720)):
    results = []

    for filename in sorted(os.listdir(image_folder)):
        if filename.lower().endswith(".png"):
            image_path = os.path.join(image_folder, filename)
            img = Image.open(image_path).resize(resize)
            page_num = int(re.search(r'\d+', filename).group()) if re.search(r'\d+', filename) else filename

            answer = vlm_answer_fn(
                img, 
                question=question,
                prompt_type=prompt_type,
                bginfo=bginfo
            )

            results.append({
                "page": page_num,
                "image_path": image_path,
                "answer": answer
            })

    print(f"âœ… æ‰€æœ‰åœ–ç‰‡å·²å®Œæˆ VLM æ¨è«–ï¼Œå…±è™•ç† {len(results)} é ")
    return results


# ===== åˆå§‹åŒ–æ¨¡å‹ =====
model = AutoModel.from_pretrained(
    'openbmb/MiniCPM-V-2_6',
    trust_remote_code=True,
    attn_implementation='sdpa',
    torch_dtype=torch.bfloat16
).eval().cuda()

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)

Prompt = {
            "Locate": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚
            ä¾‹å¦‚ï¼šã€Œæ–‡ä»¶ä¸­ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç°¡å ±çš„å“ªä¸€é ï¼Ÿã€
            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š
            {
            "answer": "è©³ç­”å…§å®¹ï¼ˆä¾‹å¦‚ï¼šã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }
          
            å¦‚æœåœ–è¡¨å‡ºç¾åœ¨å¤šé ï¼Œè«‹ç”¨æ•´æ•¸é™£åˆ—æ¨™ç¤ºæ‰€æœ‰é ç¢¼ï¼Œä¾‹å¦‚ï¼š
            {
            "answer": "ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é å’Œç¬¬12é ",
            "page": [5, 12]
            }
          
            å¦‚æœç„¡æ³•æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ï¼š
            {
            "answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ã€æ¯è‚¡ç›ˆé¤˜ã€çš„è³‡è¨Š",
            "page": []
            }''',
          
          "Compare": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            ä¾‹å¦‚ï¼šã€Œ1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº†å¤šå°‘ï¼…ï¼Ÿã€  
            ç­”ï¼š33%ã€‚è‹¥ç„¡å–®ä½è«‹å‹™å¿…åŠ ä¸Šå–®ä½ã€‚

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "33%",
            "long_answer": "1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº† 33%ã€‚",
            "page": [é ç¢¼æ•¸å­—]
            }

            è«‹æ³¨æ„ï¼š
            - short_answerï¼šåƒ…åŒ…å«æ•¸å€¼èˆ‡å–®ä½ï¼ˆå¦‚ "33%"ã€"2.6å„„å…ƒ"ï¼‰
            - long_answerï¼šå®Œæ•´èªªæ˜æ¯”è¼ƒçµæœï¼ŒåŒ…å«æ™‚é–“ã€æŒ‡æ¨™èˆ‡è®ŠåŒ–å…§å®¹
            - pageï¼šæ•´æ•¸é™£åˆ—ï¼Œä»£è¡¨ç­”æ¡ˆå‡ºè™•çš„é ç¢¼ï¼›è‹¥æœ‰å¤šé ï¼Œè«‹ä¾ç…§æ•¸å­—å¤§å°æ’åº

            è‹¥åœ–è¡¨å‡ºç¾åœ¨å¤šé ï¼Œè«‹ä½¿ç”¨æ­¤æ ¼å¼ï¼š
            {
            "short_answer": "33%",
            "long_answer": "1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº† 33%ã€‚",
            "page": [5, 12]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡è¨Šï¼Œè«‹è¼¸å‡ºï¼š
            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }
          ''',
          
          "Rank":'''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹ä¾ç…§æŒ‡æ¨™æ•¸å€¼çš„é«˜ä½é€²è¡Œæ’åºï¼Œä¸¦è¼¸å‡ºæ’åºçµæœã€‚

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "æ’åºçµæœï¼ˆä¾‹å¦‚ï¼š1Q21 > 1Q22 > 1Q24 > 1Q25 > 1Q23ï¼‰",
            "long_answer": "è©³åˆ—å„é …æ’åºåŠæ•¸å€¼ï¼ˆä¾‹å¦‚ï¼š1Q21ï¼ˆ6.31%ï¼‰ > 1Q22ï¼ˆ4.74%ï¼‰ > 1Q24ï¼ˆ4.39%ï¼‰ > 1Q25ï¼ˆ4.01%ï¼‰ > 1Q23ï¼ˆ2.69%ï¼‰ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }

            è«‹æ³¨æ„ï¼š
            - `short_answer` ç‚ºç´”æ’åºï¼Œåƒ…ä¿ç•™é …ç›®é †åºï¼Œçœç•¥æ•¸å€¼
            - `long_answer` ç‚ºå®Œæ•´æ’åºï¼Œéœ€åˆ—å‡ºæ¯å€‹é …ç›®çš„å°æ‡‰æ•¸å€¼
            - `page` ç‚ºæ•´æ•¸é™£åˆ—ï¼ŒæŒ‡å‡ºåœ–è¡¨å‡ºç¾çš„é ç¢¼ï¼Œè‹¥æœ‰å¤šé è«‹æ’åºåˆ—å‡º

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡æ–™ï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }

            ä»¥ä¸‹ç‚ºç¯„ä¾‹ï¼š

            å•é¡Œï¼šã€Œè«‹ä¾ç…§é¿éšªå¾ŒæŠ•è³‡æ”¶ç›Šç‡çš„é«˜ä½æ’åº 1Q21ï½1Q25 å„å­£è³‡æ–™ã€‚ã€

            å›ç­”ï¼š
            {
            "short_answer": "1Q21 > 1Q22 > 1Q24 > 1Q25 > 1Q23",
            "long_answer": "1Q21ï¼ˆ6.31%ï¼‰ > 1Q22ï¼ˆ4.74%ï¼‰ > 1Q24ï¼ˆ4.39%ï¼‰ > 1Q25ï¼ˆ4.01%ï¼‰ > 1Q23ï¼ˆ2.69%ï¼‰",
            "page": [25]
            }
            ''',

          "Trend": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            ç•¶å•é¡Œè©¢å•æŸæŒ‡æ¨™åœ¨ä¸€æ®µæœŸé–“å…§çš„è®ŠåŒ–è¶¨å‹¢æ™‚ï¼Œè«‹æ ¹æ“šå¯¦éš›æ•¸æ“šæƒ…æ³ç°¡æ½”å›ç­”ï¼š
            - è‹¥æ•¸å€¼æ˜é¡¯ä¸Šå‡ï¼Œè«‹å›ç­”ã€Œä¸Šå‡ã€
            - è‹¥æ•¸å€¼æ˜é¡¯ä¸‹é™ï¼Œè«‹å›ç­”ã€Œä¸‹æ»‘ã€æˆ–ã€Œä¸‹é™ã€
            - è‹¥æ•¸å€¼è®ŠåŒ–ä¸å¤§ï¼Œè«‹å›ç­”ã€ŒæŒå¹³ã€
            - è‹¥è®ŠåŒ–æœ‰è½‰æŠ˜ï¼Œè«‹å¦‚å¯¦å›ç­”ã€Œå…ˆå‡å¾Œé™ã€ã€ã€Œå…ˆé™å¾Œå‡ã€ç­‰

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "è¶¨å‹¢ç°¡è¿°ï¼ˆä¾‹å¦‚ï¼šä¸‹æ»‘ï¼‰",
            "long_answer": "å…·é«”èªªæ˜è¶¨å‹¢èˆ‡æ•¸å€¼è®ŠåŒ–ï¼ˆä¾‹å¦‚ï¼šæŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }

            è‹¥å‡ºç¾å¤šé ï¼Œè«‹ä¾æ•¸å­—å¤§å°æ’åºï¼š

            {
            "short_answer": "ä¸‹æ»‘",
            "long_answer": "æŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰",
            "page": [10, 11]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡æ–™ï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }

            ç¯„ä¾‹é¡Œç›®èˆ‡å›ç­”ï¼š

            å•é¡Œï¼šã€Œæµ·å¤–ç²åˆ©ä½”æ¯”åœ¨ FY20 è‡³ FY23 å‘ˆç¾ä»€éº¼è¶¨å‹¢ï¼Ÿã€

            å›ç­”ï¼š
            {
            "short_answer": "ä¸‹æ»‘",
            "long_answer": "æŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰",
            "page": [10]
            }
          ''',
          
          "Extract":'''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            è«‹ä»¥æœ€ç°¡å–®çš„æ–¹å¼å›ç­”ï¼š  
            - è‹¥ç­”æ¡ˆç‚ºå–®ä¸€é …ç›®ï¼Œè«‹ç›´æ¥å›ç­”é …ç›®åç¨±æˆ–æ•¸å€¼  
            - è‹¥éœ€åˆ—å‡ºå¤šå€‹é …ç›®ï¼Œè«‹ä¾ç…§ä»¥ä¸‹è¦å‰‡åˆ†éš”ï¼š  
            - ä¸­æ–‡é …ç›®ï¼šä½¿ç”¨é “è™Ÿï¼ˆã€ï¼‰  
            - è‹±æ–‡é …ç›®ï¼šä½¿ç”¨é€—è™Ÿï¼ˆ,ï¼‰

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "æ“·å–å‡ºçš„é …ç›®ï¼ˆæœ€ç°¡æ ¼å¼ï¼Œä½¿ç”¨æ­£ç¢ºåˆ†éš”ç¬¦è™Ÿï¼‰",
            "long_answer": "åŒ…å«ä¸Šä¸‹æ–‡èªªæ˜çš„å®Œæ•´å›ç­”",
            "page": [é ç¢¼æ•¸å­—]
            }

            è‹¥å‡ºç¾å¤šé ï¼Œè«‹ä¾é ç¢¼å¤§å°æ’åºï¼Œä¾‹å¦‚ï¼š

            {
            "short_answer": "ä¿¡ç”¨å¡ã€è²¡å¯Œç®¡ç†ã€å¤–åŒ¯ç®¡ç†ã€è¯è²¸ã€å…¶ä»–",
            "long_answer": "æ‰‹çºŒè²»æ·¨æ”¶ç›Šçš„æ§‹æˆé …ç›®åŒ…æ‹¬ï¼šä¿¡ç”¨å¡ã€è²¡å¯Œç®¡ç†ã€å¤–åŒ¯ç®¡ç†ã€è¯è²¸åŠå…¶ä»–ã€‚",
            "page": [9, 11]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡è¨Šï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }
            '''}
bginfo = "å¤§æ¨™é¡Œé€šå˜—åœ¨å·¦ä¸Šè§’ï¼Œé ç¢¼ä½æ–¼å³ä¸‹è§’ã€‚é•·æ¢åœ–çš„æ¨™é¡Œä½æ–¼åœ–ç‰‡æ­£ä¸Šæ–¹ã€‚"

# ===== VLM å›ç­”å‡½å¼ =====
def vlm_answer(image: Image.Image, question: str, prompt_type: str) -> str:
    prompt = Prompt[prompt_type] + f"\n\n{bginfo}\n\n"
    full_question = f"{question}\n\n{prompt}"
    imgs = [image]
    msgs = [{'role': 'user', 'content': imgs + [full_question]}]

    with torch.no_grad():
        answer = model.chat(
            image=None,
            msgs=msgs,
            tokenizer=tokenizer,
            sampling=True,
            temperature=0.2,
            max_new_tokens=256
        )
    return answer

def run_vlm_multi_images(matched_pages, question, vlm_answer_fn, prompt_type, resize=(720, 720)):
    if not matched_pages:
        return []

    # æŠŠå¤šé åœ–ç‰‡ä¸€æ¬¡é€é€²æ¨¡å‹
    imgs = [Image.open(p['image_path']).resize(resize) for p in matched_pages]
    stitched_img = stitch_images([p['image_path'] for p in matched_pages])
    answer = vlm_answer_fn(stitched_img, question, prompt_type)
    print(f"ğŸ“„ å¤šé åˆä½µå›ç­”ï¼š{answer}")

    return [{"page": p['page'], "image_path": p['image_path'], "answer": answer} for p in matched_pages]


# ===== ç¯©é¸ç›¸é—œé é¢ =====
def filter_related_pages_by_vlm(image_folder, question, vlm_answer_fn, resize=(720, 720)):
    related_pages = []
    
    positive_keywords = ["æ˜¯", "ç›¸é—œ", "æåˆ°", "é¡¯ç¤º", "åŒ…å«"]
    negative_keywords = ["å¦", "ä¸ç›¸é—œ", "ç„¡é—œ", "ç„¡æ³•å–å¾—", "ä¸åŒ…å«", "æ²’æœ‰æåˆ°", "ä¸æ¸…æ¥š"]

    for filename in sorted(os.listdir(image_folder)):
        if filename.lower().endswith(".png"):
            image_path = os.path.join(image_folder, filename)
            img = Image.open(image_path).resize(resize)

            # è§£æé ç¢¼
            match = re.search(r'\d+', filename)
            page_num = int(match.group()) if match else filename

            # å»ºç«‹ prompt
            relevance_question = (
                f"è«‹ä½ åˆ¤æ–·é€™ä¸€é æ˜¯å¦èˆ‡ä»¥ä¸‹å•é¡Œæœ‰é—œï¼šã€Œ{question}ã€ã€‚"
                "å¦‚æœæœ‰é—œï¼Œè«‹å›ç­”ã€Œæ˜¯ã€ï¼›å¦‚æœç„¡é—œæˆ–ç„¡æ³•åˆ¤æ–·ï¼Œè«‹å›ç­”ã€Œå¦ã€ã€‚"
            )
            
            # ç™¼é€çµ¦ VLM
            answer = vlm_answer_fn(img, relevance_question, prompt_type="Trend").strip()
            print(f"ğŸ§ é  {page_num} â†’ å›ç­”ï¼šã€Œ{answer}ã€")

            # åˆ¤æ–·æ˜¯å¦ç‚ºç›¸é—œé 
            answer_lower = answer.lower()
            has_positive = any(kw in answer_lower for kw in positive_keywords)
            has_negative = any(kw in answer_lower for kw in negative_keywords)

            if has_positive and not has_negative:
                related_pages.append({"page": page_num, "image_path": image_path})
    
    return related_pages

# ===== åŸ·è¡Œ VLM å•å®Œæ•´å•é¡Œ =====
def run_vlm(matched_pages, question, vlm_answer_fn, prompt_type, resize=(720, 720)):
    results = []
    for page_info in matched_pages:
        img = Image.open(page_info['image_path']).resize(resize)
        answer = vlm_answer_fn(img, question, prompt_type)
        print(f"ğŸ“„ ç¬¬ {page_info['page']} é  â†’ å›ç­”ï¼š{answer}")
        results.append({"page": page_info['page'], "image_path": page_info['image_path'], "answer": answer})
    return results


# ===== æ•´ç†æœ€çµ‚ç­”æ¡ˆ =====
def summarize_answers(results):
    if not results:
        return {"final_answer": "æ‰¾ä¸åˆ°ä»»ä½•ç›¸é—œè³‡è¨Š", "pages": []}
    best = max(results, key=lambda r: len(r["answer"]))
    return {"final_answer": best["answer"], "pages": [r["page"] for r in results]}


def read_column_as_list(csv_path: str, column_name: str) -> list:
    result = []
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        if column_name not in reader.fieldnames:
            raise ValueError(f"âŒ æ‰¾ä¸åˆ°æ¬„ä½ï¼š{column_name}")
        for row in reader:
            value = row[column_name].strip()
            if value:
                result.append(value)
    return result

def exact_match_score(prediction: str, reference: str) -> int:
    """
    Returns 1 if prediction exactly matches reference (ignoring space and character form), else 0.
    """
    def normalize(text):
        text = unicodedata.normalize("NFKC", text)
        return re.sub(r"\s+", "", text.strip())
    return int(normalize(prediction) == normalize(reference))

def llm_judge_score(question: str, reference: str, prediction: str, model_name: str, openai_api_key: str) -> int:
    openai.api_key = openai_api_key

    prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI è©•ä¼°åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å•é¡Œã€æ¨™æº–ç­”æ¡ˆèˆ‡æ¨¡å‹å›è¦†ï¼Œçµ¦å‡º 1ï½5 çš„ç›¸ä¼¼åº¦åˆ†æ•¸ã€‚
        1ï¼šå®Œå…¨ä¸ç›¸é—œï¼Œ5ï¼šå®Œå…¨ä¸€è‡´ã€‚

        ---
        å•é¡Œï¼š
        {question}

        æ¨™æº–ç­”æ¡ˆï¼š
        {reference}

        æ¨¡å‹ï¼ˆ{model_name}ï¼‰çš„å›ç­”ï¼š
        {prediction}

        è«‹ä½ åªå›ç­”ä¸€å€‹æ•´æ•¸åˆ†æ•¸ï¼ˆ1â€“5ï¼‰ï¼Œä¸è¦é™„åŠ ä»»ä½•èªªæ˜ã€‚
    """

    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=10
        )
        score = int(re.search(r"[1-5]", response.choices[0].message.content).group())
        return score
    except Exception as e:
        print(f"âš ï¸ GPT è©•åˆ†éŒ¯èª¤ï¼š{e}")
        return -1

def evaluate_with_llm(csv_path, model_results, model_name, openai_api_key, output_csv="final_eval_output.csv"):
    df = pd.read_csv(csv_path)
    results = []

    for item in model_results:
        question = item["question"]
        raw = item["answer"]

        try:
            pred_json = eval(raw)
            short_pred = pred_json.get("short_answer", raw)
            long_pred = pred_json.get("long_answer", raw)
        except:
            short_pred = raw
            long_pred = raw

        ref_row = df[df["Question"] == question]
        if ref_row.empty:
            print(f"âŒ æ‰¾ä¸åˆ°å°æ‡‰é¡Œç›®ï¼š{question}")
            continue

        ref_data = ref_row.iloc[0]
        short_ref = ref_data["Short Answer"]
        long_ref = ref_data["Long Answer"]
        q_type = ref_data.get("Chart Type", "Unknown")

        em = exact_match_score(short_pred, short_ref)
        llm_score = llm_judge_score(question, long_ref, long_pred, model_name, openai_api_key)

        results.append({
            "Type": q_type,
            "Question": question,
            "Short Answer": short_ref,
            "Long Answer": long_ref,
            "Prediction": long_pred,
            "Exact Match Score": em,
            "LLM Judge Score": llm_score,
            "Model": model_name
        })

    final_df = pd.DataFrame(results)
    final_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… è©•ä¼°å®Œæˆï¼Œå·²è¼¸å‡ºè‡³ï¼š{output_csv}")
    return final_df





if __name__ == '__main__':
    image_folder = "./data/pdf_img/1Q25_MP_Chinese_Vupload/"
    csv_path = "./QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - å®šä½å•é¡Œ.csv"
    question_col = "Question"
    prompt_type = "Locate"  # Compare, Extract, Locate, Rank, Trend

    questions = read_column_as_list(csv_path, question_col)
    all_model_results = []

    for idx, question in enumerate(questions, 1):
        print(f"\nğŸŸ¢ å•é¡Œ {idx}/{len(questions)}ï¼š{question}")

        # 1. ç¯©é¸ç›¸é—œé 
        related_pages = filter_related_pages_by_vlm(
            image_folder=image_folder,
            question=question,
            vlm_answer_fn=vlm_answer
        )

        # 2. æ ¹æ“šé¡Œå‹æ±ºå®šå¦‚ä½•å•
        if prompt_type == "Locate":
            # å¤šé åˆä½µé€é€²æ¨¡å‹
            results = run_vlm_multi_images(
                matched_pages=related_pages,
                question=question,
                vlm_answer_fn=vlm_answer,
                prompt_type=prompt_type
            )
        else:
            # åŸæœ¬çš„é€é æ–¹å¼
            results = run_vlm(
                matched_pages=related_pages,
                question=question,
                vlm_answer_fn=vlm_answer,
                prompt_type=prompt_type
            )

        # 3. æ•´ç†ç­”æ¡ˆ
        final = summarize_answers(results)
        print("\nğŸ¯ æœ€çµ‚ç­”æ¡ˆï¼š")
        print(final)

        all_model_results.append({
            "question": question,
            "answer": final["final_answer"]
        })

    # 4. è©•ä¼°
    evaluate_with_llm(
        csv_path=csv_path,
        model_results=all_model_results,
        model_name="MiniCPM-V-2_6",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        output_csv="vlm_eval_with_llm_Locate.csv"
    )


