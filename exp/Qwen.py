import os
import csv
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from rouge_score import rouge_scorer
import torch
import time
import io
import base64
import openai


def stitch_images(image_paths):
    images = [Image.open(p).convert("RGB").resize((720, 720)) for p in image_paths]
    total_width = sum(img.width for img in images)
    max_height = max(img.height for img in images)
    stitched = Image.new("RGB", (total_width, max_height))
    x_offset = 0
    for img in images:
        stitched.paste(img, (x_offset, 0))
        x_offset += img.width
    return stitched


Prompt = {
    "Locate": (
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èäººå“¡ï¼Œè«‹ä½ æ“·å–åœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
        "è«‹ç°¡ç­”ï¼Œä¾‹å¦‚ï¼šã€Œxxxåœ¨ç¬¬å¹¾é ï¼Ÿã€æˆ–ã€Œxxxåœ¨å“ªè£¡ã€è«‹å›ç­” ã€Œç¬¬yé ã€æˆ–æœ‰è¶…éä¸€é å¯ä»¥å›ç­”ã€Œç¬¬yé å’Œã€ç¬¬zé ã€"
        "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œ"
    ),
    "Compare": (
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èäººå“¡ï¼Œè«‹ä½ æ“·å–åœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
        "è«‹ç°¡ç­”ï¼Œä¾‹å¦‚:å•é¡Œå•æ•¸å­—ç›¸é—œçš„å•é¡Œä¸¦ä¸”æœ‰å–®ä½ï¼Œè«‹ç›´æ¥å›ç­”æ•¸å­—å³å¯ã€‚è‹¥ç„¡å–®ä½è«‹å‹™å¿…åŠ ä¸Šå–®ä½ã€‚"
        "ç¹é«”ä¸­æ–‡å›ç­”"
    ),
    "Rank": (
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èäººå“¡ï¼Œè«‹ä½ æ“·å–åœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
        "å¦‚æœé‡åˆ°æœ€é«˜æˆ–æ˜¯æœ€ä½è«‹ç›´æ¥ä»¥æ–‡å­—å›ç­”æœ€é«˜æˆ–æ˜¯æœ€ä½çš„é …ç›®ï¼Œå¦‚æœæ˜¯è«‹ä»¥é«˜ä½æ’åºè«‹ä»¥å¤§æ–¼ï¼ˆ>ï¼‰æˆ–æ˜¯å°æ–¼ï¼ˆ<ï¼‰ç¬¦è™Ÿè¡¨ç¤ºã€‚ä¾‹å¦‚ï¼šã€ŒA>B>Cã€ï¼Œåä¹‹äº¦ç„¶ã€‚"
        "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œ"
    ),
    "Trend": (
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èäººå“¡ï¼Œè«‹ä½ æ“·å–åœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
        "å•è¶¨å‹¢æˆ–æ˜¯è®ŠåŒ–æ™‚å¯ä»¥ä¸Šå‡ä¸‹é™æˆ–æ˜¯æŒå¹³ç‚ºç­”æ¡ˆï¼Œä½†æœ‰æ™‚æœƒå‡ºç¾å…ˆå‡å¾Œé™æˆ–æ˜¯å…ˆå°‡å¾Œå‡ï¼Œè«‹ä»¥å¯¦éš›æƒ…æ³ç‚ºä¸»ã€‚"
        "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
    ),
    "Extract": (
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èäººå“¡ï¼Œè«‹ä½ æ“·å–åœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆ"
        "è«‹ä»¥æœ€ç°¡å–®çš„æ–¹å¼å›ç­”ï¼Œå¦‚éœ€åˆ—å‡ºå¤šå€‹é …ç›®ï¼Œè‹¥ç‚ºä¸­æ–‡è«‹ä»¥é “è™Ÿï¼ˆã€ï¼‰åˆ†éš”ï¼Œè‹¥ç‚ºè‹±æ–‡è«‹ä»¥é€—è™Ÿï¼ˆ,ï¼‰åˆ†éš”ã€‚"
        "ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
    ),
}


def run_openai_batch(image_files, question, batch_size, OPENAI_API_KEY, prompt_type="Locate"):
    openai.api_key = OPENAI_API_KEY
    full_question = f"{question}\n\n{Prompt[prompt_type]}"
    summaries = []

    for i in range(0, len(image_files), batch_size):
        batch_paths = image_files[i:i + batch_size]
        stitched_image = stitch_images(batch_paths)

        buffer = io.BytesIO()
        stitched_image.save(buffer, format="JPEG")
        image_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": full_question},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                        ]
                    }
                ],
                max_tokens=256,
                temperature=0.2
            )
            summaries.append(response.choices[0].message.content.strip())
        except Exception as e:
            summaries.append(f"(ERROR: {e})")

    return summaries


def run_vlm_batch(image_files, question, batch_size, processor, model, prompt_type="Rank"):
    full_question = f"{question}\n\n{Prompt[prompt_type]}"
    summaries = []
    for i in range(0, len(image_files), batch_size):
        batch_paths = image_files[i:i + batch_size]
        images = [Image.open(p).convert("RGB").resize((720, 720)) for p in batch_paths]

        message = [{
            "role": "user",
            "content": ([{"type": "image", "image": img} for img in images] +
                        [{"type": "text", "text": full_question}])
        }]

        try:
            text = processor.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(message)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                return_tensors="pt",
                padding=True
            ).to("cuda")

            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.2)
            trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]
            decoded = processor.batch_decode(trimmed, skip_special_tokens=True)[0]
            summaries.append(decoded.strip())
        except torch.cuda.OutOfMemoryError:
            summaries.append("ï¼ˆOOMï¼‰")
            torch.cuda.empty_cache()
    return summaries


def summarize_final_answer(summaries, question, processor, model):
    combined = "\n".join([f"æ‰¹æ¬¡ {i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    final_prompt = f"ä»¥ä¸‹æ˜¯æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦ï¼š\n{combined}\n\nè«‹æ ¹æ“šä»¥ä¸‹æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦å…§å®¹ï¼Œæ‰¾å‡ºæœ€åˆç†çš„å…·é«”ç­”æ¡ˆï¼Œåªéœ€ç›´æ¥å›ç­”çµæœå³å¯ï¼Œæ•¬è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š{question}"

    message = [{"role": "user", "content": [{"type": "text", "text": final_prompt}]}]
    text = processor.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], return_tensors="pt").to("cuda")

    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=256, temperature=0.2)
    trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, output_ids)]
    return processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()


def evaluate_answer(pred, ref):
    pred, ref = pred.lower().strip(), ref.lower().strip()
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge = scorer.score(ref, pred)['rougeL'].fmeasure
    hit = int(any(word in pred for word in ref.split()))
    return {"rougeL_f1": round(rouge, 4), "substring_hit": hit}


def process_csv_questions(csv_path, image_folder, processor, model, OPENAI_API_KEY, batch_size):
    results = []

    image_files = sorted([
        os.path.join(image_folder, f)
        for f in os.listdir(image_folder)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ])

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, 1):
            question = row['Question']
            reference = row['Short Answer']
            print(f"\n============= å•é¡Œ {idx}: {question} =============")

            qwen_summaries = run_vlm_batch(image_files, question, batch_size, processor, model)
            qwen_prediction = summarize_final_answer(qwen_summaries, question, processor, model)
            print(f"ğŸ¤– Qwen å›ç­”ï¼š{qwen_prediction}")

            openai_summaries = run_openai_batch(image_files, question, batch_size, OPENAI_API_KEY)
            openai_prediction = summarize_final_answer(openai_summaries, question, processor, model)
            print(f"ğŸ§  OpenAI å›ç­”ï¼š{openai_prediction}")

            qwen_metrics = evaluate_answer(qwen_prediction, reference)
            openai_metrics = evaluate_answer(openai_prediction, reference)

            results.append({
                "id": idx,
                "question": question,
                "reference": reference,
                "qwen_prediction": qwen_prediction,
                "openai_prediction": openai_prediction,
                "evaluation": {
                    "qwen_rouge": qwen_metrics['rougeL_f1'],
                    "qwen_hit": qwen_metrics['substring_hit'],
                    "openai_rouge": openai_metrics['rougeL_f1'],
                    "openai_hit": openai_metrics['substring_hit']
                }
            })

    return results


if __name__ == '__main__':
    from dotenv import load_dotenv
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct", torch_dtype="auto", device_map="auto")
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
    csv_path = "./QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - å®šä½å•é¡Œ.csv"
    image_folder = "./output_images_fitz"

    start_time = time.time()
    results = process_csv_questions(csv_path, image_folder, processor, model, OPENAI_API_KEY, batch_size=3)
    end_time = time.time()
    elapsed_time = end_time - start_time
    minutes, seconds = divmod(elapsed_time, 60)

    print("\nğŸ“‹ æ¯ä¸€é¡Œè©•ä¼°çµæœï¼š")
    for r in results:
        rouge_qwen = r['evaluation']['qwen_rouge']
        hit_qwen = r['evaluation']['qwen_hit']
        rouge_openai = r['evaluation']['openai_rouge']
        hit_openai = r['evaluation']['openai_hit']
        print(f"ID {r['id']:>2} | Qwen ROUGE: {rouge_qwen:.4f}, Hit: {hit_qwen} | OpenAI ROUGE: {rouge_openai:.4f}, Hit: {hit_openai}")

    print("\nğŸ“Š Evaluation Summary:")
    total = len(results)
    avg = lambda key: round(sum(r['evaluation'][key] for r in results) / total, 4) if total else 0.0
    print(f"ğŸ”¹ Qwen - Avg ROUGE: {avg('qwen_rouge')}, Avg Hit: {avg('qwen_hit')}")
    print(f"ğŸ”¹ OpenAI - Avg ROUGE: {avg('openai_rouge')}, Avg Hit: {avg('openai_hit')}")
    print(f"\nâ±ï¸ Total Runtime: {int(minutes)} min {int(seconds)} sec")
