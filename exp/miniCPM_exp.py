import torch
import os
import csv
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from rouge_score import rouge_scorer
from dotenv import load_dotenv
load_dotenv()
import re
import unicodedata
import openai
import base64
import io
import time

# ===== OpenAI gpt-4o api =====
def stitch_images(image_paths):
    images = [Image.open(p).convert("RGB").resize((720, 720)) for p in image_paths]
    total_width = sum(img.width for img in images)
    max_height = max(img.height for img in images)
    stitched = Image.new("RGB", (total_width, max_height))
    x_offset = 0
    for img in images:
        stitched.paste(img, (x_offset, 0))
        x_offset += img.width
    return stitched



def encode_image(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode("utf-8")

def openai_inference(image_obj, question, OPENAI_API_KEY, MAX_NEW_TOKEN=256, TEMPERATURE=0.2, repetition_penalty=1.05):
    openai.api_key = OPENAI_API_KEY
    buffer = io.BytesIO()
    image_obj.save(buffer, format="JPEG")
    image_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ]
            }
        ],
        max_tokens=MAX_NEW_TOKEN,
        temperature=TEMPERATURE,
        frequency_penalty=repetition_penalty
    )

    return response.choices[0].message.content.strip()


# ===== åˆå§‹åŒ–æ¨¡å‹ =====
model = AutoModel.from_pretrained(
    'openbmb/MiniCPM-V-2_6',
    trust_remote_code=True,
    attn_implementation='sdpa',
    torch_dtype=torch.bfloat16
).eval().cuda()

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)

# ===== è¼‰å…¥åœ–ç‰‡æ‰¹æ¬¡ =====
def load_image_batches(folder_path, batch_size):
    image_files = sorted([
        os.path.join(folder_path, f)
        for f in os.listdir(folder_path)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ])
    for i in range(0, len(image_files), batch_size):
        paths = image_files[i:i + batch_size]
        images = [Image.open(p).convert("RGB").resize((720, 720)) for p in paths]
        yield images


Prompt = {"Locate": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–ç‰‡ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚
            ä¾‹å¦‚ï¼šã€Œæ–‡ä»¶ä¸­ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç°¡å ±çš„å“ªä¸€é ï¼Ÿã€
            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š
            {
            "answer": "è©³ç­”å…§å®¹ï¼ˆä¾‹å¦‚ï¼šã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }
          
            å¦‚æœåœ–è¡¨å‡ºç¾åœ¨å¤šé ï¼Œè«‹ç”¨æ•´æ•¸é™£åˆ—æ¨™ç¤ºæ‰€æœ‰é ç¢¼ï¼Œä¾‹å¦‚ï¼š
            {
            "answer": "ã€æ¯è‚¡ç›ˆé¤˜ã€çš„æ•¸æ“šåœ–å‡ºç¾åœ¨ç¬¬5é å’Œç¬¬12é ",
            "page": [5, 12]
            }
          
            å¦‚æœç„¡æ³•æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ï¼š
            {
            "answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ã€æ¯è‚¡ç›ˆé¤˜ã€çš„è³‡è¨Š",
            "page": []
            }''',
          
          "Compare": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            ä¾‹å¦‚ï¼šã€Œ1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº†å¤šå°‘ï¼…ï¼Ÿã€  
            ç­”ï¼š33%ã€‚è‹¥ç„¡å–®ä½è«‹å‹™å¿…åŠ ä¸Šå–®ä½ã€‚

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "33%",
            "long_answer": "1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº† 33%ã€‚",
            "page": [é ç¢¼æ•¸å­—]
            }

            è«‹æ³¨æ„ï¼š
            - short_answerï¼šåƒ…åŒ…å«æ•¸å€¼èˆ‡å–®ä½ï¼ˆå¦‚ "33%"ã€"2.6å„„å…ƒ"ï¼‰
            - long_answerï¼šå®Œæ•´èªªæ˜æ¯”è¼ƒçµæœï¼ŒåŒ…å«æ™‚é–“ã€æŒ‡æ¨™èˆ‡è®ŠåŒ–å…§å®¹
            - pageï¼šæ•´æ•¸é™£åˆ—ï¼Œä»£è¡¨ç­”æ¡ˆå‡ºè™•çš„é ç¢¼ï¼›è‹¥æœ‰å¤šé ï¼Œè«‹ä¾ç…§æ•¸å­—å¤§å°æ’åº

            è‹¥åœ–è¡¨å‡ºç¾åœ¨å¤šé ï¼Œè«‹ä½¿ç”¨æ­¤æ ¼å¼ï¼š
            {
            "short_answer": "33%",
            "long_answer": "1Q25 èˆ‡ 1Q24 ç›¸æ¯”ï¼Œæ‰‹çºŒè²»æ·¨æ”¶ç›Šç¸½é¡æˆé•·äº† 33%ã€‚",
            "page": [5, 12]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡è¨Šï¼Œè«‹è¼¸å‡ºï¼š
            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }
          ''',
          
          "Rank":'''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹ä¾ç…§æŒ‡æ¨™æ•¸å€¼çš„é«˜ä½é€²è¡Œæ’åºï¼Œä¸¦è¼¸å‡ºæ’åºçµæœã€‚

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "æ’åºçµæœï¼ˆä¾‹å¦‚ï¼š1Q21 > 1Q22 > 1Q24 > 1Q25 > 1Q23ï¼‰",
            "long_answer": "è©³åˆ—å„é …æ’åºåŠæ•¸å€¼ï¼ˆä¾‹å¦‚ï¼š1Q21ï¼ˆ6.31%ï¼‰ > 1Q22ï¼ˆ4.74%ï¼‰ > 1Q24ï¼ˆ4.39%ï¼‰ > 1Q25ï¼ˆ4.01%ï¼‰ > 1Q23ï¼ˆ2.69%ï¼‰ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }

            è«‹æ³¨æ„ï¼š
            - `short_answer` ç‚ºç´”æ’åºï¼Œåƒ…ä¿ç•™é …ç›®é †åºï¼Œçœç•¥æ•¸å€¼
            - `long_answer` ç‚ºå®Œæ•´æ’åºï¼Œéœ€åˆ—å‡ºæ¯å€‹é …ç›®çš„å°æ‡‰æ•¸å€¼
            - `page` ç‚ºæ•´æ•¸é™£åˆ—ï¼ŒæŒ‡å‡ºåœ–è¡¨å‡ºç¾çš„é ç¢¼ï¼Œè‹¥æœ‰å¤šé è«‹æ’åºåˆ—å‡º

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡æ–™ï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }

            ä»¥ä¸‹ç‚ºç¯„ä¾‹ï¼š

            å•é¡Œï¼šã€Œè«‹ä¾ç…§é¿éšªå¾ŒæŠ•è³‡æ”¶ç›Šç‡çš„é«˜ä½æ’åº 1Q21ï½1Q25 å„å­£è³‡æ–™ã€‚ã€

            å›ç­”ï¼š
            {
            "short_answer": "1Q21 > 1Q22 > 1Q24 > 1Q25 > 1Q23",
            "long_answer": "1Q21ï¼ˆ6.31%ï¼‰ > 1Q22ï¼ˆ4.74%ï¼‰ > 1Q24ï¼ˆ4.39%ï¼‰ > 1Q25ï¼ˆ4.01%ï¼‰ > 1Q23ï¼ˆ2.69%ï¼‰",
            "page": [25]
            }
            ''',

          "Trend": '''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            ç•¶å•é¡Œè©¢å•æŸæŒ‡æ¨™åœ¨ä¸€æ®µæœŸé–“å…§çš„è®ŠåŒ–è¶¨å‹¢æ™‚ï¼Œè«‹æ ¹æ“šå¯¦éš›æ•¸æ“šæƒ…æ³ç°¡æ½”å›ç­”ï¼š
            - è‹¥æ•¸å€¼æ˜é¡¯ä¸Šå‡ï¼Œè«‹å›ç­”ã€Œä¸Šå‡ã€
            - è‹¥æ•¸å€¼æ˜é¡¯ä¸‹é™ï¼Œè«‹å›ç­”ã€Œä¸‹æ»‘ã€æˆ–ã€Œä¸‹é™ã€
            - è‹¥æ•¸å€¼è®ŠåŒ–ä¸å¤§ï¼Œè«‹å›ç­”ã€ŒæŒå¹³ã€
            - è‹¥è®ŠåŒ–æœ‰è½‰æŠ˜ï¼Œè«‹å¦‚å¯¦å›ç­”ã€Œå…ˆå‡å¾Œé™ã€ã€ã€Œå…ˆé™å¾Œå‡ã€ç­‰

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "è¶¨å‹¢ç°¡è¿°ï¼ˆä¾‹å¦‚ï¼šä¸‹æ»‘ï¼‰",
            "long_answer": "å…·é«”èªªæ˜è¶¨å‹¢èˆ‡æ•¸å€¼è®ŠåŒ–ï¼ˆä¾‹å¦‚ï¼šæŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰ï¼‰",
            "page": [é ç¢¼æ•¸å­—]
            }

            è‹¥å‡ºç¾å¤šé ï¼Œè«‹ä¾æ•¸å­—å¤§å°æ’åºï¼š

            {
            "short_answer": "ä¸‹æ»‘",
            "long_answer": "æŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰",
            "page": [10, 11]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡æ–™ï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }

            ç¯„ä¾‹é¡Œç›®èˆ‡å›ç­”ï¼š

            å•é¡Œï¼šã€Œæµ·å¤–ç²åˆ©ä½”æ¯”åœ¨ FY20 è‡³ FY23 å‘ˆç¾ä»€éº¼è¶¨å‹¢ï¼Ÿã€

            å›ç­”ï¼š
            {
            "short_answer": "ä¸‹æ»‘",
            "long_answer": "æŒçºŒä¸‹æ»‘ï¼Œå¾ 45.9%ï¼ˆFY20ï¼‰ä¸€è·¯ä¸‹é™è‡³ 8.9%ï¼ˆFY23ï¼‰",
            "page": [10]
            }
          ''',
          
          "Extract":'''è«‹ä½ æ ¹æ“šåœ–ç‰‡ä¸­çš„æ•¸æ“šåœ–è¡¨ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ èƒ½å¾åœ–è¡¨ä¸­æ‰¾åˆ°è³‡è¨Šï¼Œè«‹çµ¦å‡ºæ˜ç¢ºä¸”å…·é«”çš„ç­”æ¡ˆã€‚

            è«‹ä»¥æœ€ç°¡å–®çš„æ–¹å¼å›ç­”ï¼š  
            - è‹¥ç­”æ¡ˆç‚ºå–®ä¸€é …ç›®ï¼Œè«‹ç›´æ¥å›ç­”é …ç›®åç¨±æˆ–æ•¸å€¼  
            - è‹¥éœ€åˆ—å‡ºå¤šå€‹é …ç›®ï¼Œè«‹ä¾ç…§ä»¥ä¸‹è¦å‰‡åˆ†éš”ï¼š  
            - ä¸­æ–‡é …ç›®ï¼šä½¿ç”¨é “è™Ÿï¼ˆã€ï¼‰  
            - è‹±æ–‡é …ç›®ï¼šä½¿ç”¨é€—è™Ÿï¼ˆ,ï¼‰

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š

            {
            "short_answer": "æ“·å–å‡ºçš„é …ç›®ï¼ˆæœ€ç°¡æ ¼å¼ï¼Œä½¿ç”¨æ­£ç¢ºåˆ†éš”ç¬¦è™Ÿï¼‰",
            "long_answer": "åŒ…å«ä¸Šä¸‹æ–‡èªªæ˜çš„å®Œæ•´å›ç­”",
            "page": [é ç¢¼æ•¸å­—]
            }

            è‹¥å‡ºç¾å¤šé ï¼Œè«‹ä¾é ç¢¼å¤§å°æ’åºï¼Œä¾‹å¦‚ï¼š

            {
            "short_answer": "ä¿¡ç”¨å¡ã€è²¡å¯Œç®¡ç†ã€å¤–åŒ¯ç®¡ç†ã€è¯è²¸ã€å…¶ä»–",
            "long_answer": "æ‰‹çºŒè²»æ·¨æ”¶ç›Šçš„æ§‹æˆé …ç›®åŒ…æ‹¬ï¼šä¿¡ç”¨å¡ã€è²¡å¯Œç®¡ç†ã€å¤–åŒ¯ç®¡ç†ã€è¯è²¸åŠå…¶ä»–ã€‚",
            "page": [9, 11]
            }

            è‹¥ç„¡æ³•å¾åœ–è¡¨ä¸­å–å¾—è³‡è¨Šï¼Œè«‹å›ç­”ï¼š

            {
            "short_answer": "ç„¡æ³•å–å¾—",
            "long_answer": "ç„¡æ³•å¾åœ–è¡¨ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Š",
            "page": []
            }
            '''}

# ===== å–®æ¬¡æå•ï¼ˆé™„åŠ å¤±æ•—æç¤ºï¼‰=====
def ask(images, question , prompt_type="Extract"):
    prompt = Prompt[prompt_type]
    full_question = f"{question}\n\n{prompt}"
    msgs = [{'role': 'user', 'content': images + [full_question]}]
    with torch.no_grad():
        answer = model.chat(
            image=None, 
            msgs=msgs, 
            tokenizer=tokenizer,
            sampling=True,             
            temperature=0.2,            
            max_new_tokens=256)          
    return answer

# ===== æ•´åˆæ‘˜è¦å†æ¬¡æå• =====
def summarize_all(summaries, question):
    combined = "\n".join([f"æ‰¹æ¬¡{i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    final_prompt = f"ä»¥ä¸‹æ˜¯æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦ï¼š\n{combined}\n\nè«‹æ ¹æ“šä»¥ä¸‹æ¯æ‰¹åœ–ç‰‡çš„æ‘˜è¦å…§å®¹ï¼Œæ‰¾å‡ºæœ€åˆç†çš„å…·é«”ç­”æ¡ˆï¼Œåªéœ€ç›´æ¥å›ç­”çµæœå³å¯ï¼Œæ•¬è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š{question}"
    return ask([], final_prompt)

# ===== è©•ä¼°æŒ‡æ¨™ =====
def substring_hit(pred, ref):
    pred = pred.lower().strip()
    ref = ref.lower().strip()
    ref_keywords = ref.split()
    
    for word in ref_keywords:
        if re.search(r'\b{}\b'.format(re.escape(word)), pred):
            return 1
    return 0


def evaluate_answer(pred, ref):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_l_score = scorer.score(ref, pred)['rougeL'].fmeasure
    hit_score = substring_hit(pred, ref)
    return {
        "rougeL_f1": round(rouge_l_score, 4),
        "substring_hit": hit_score,
    }

# ===== è™•ç† CSV å•é¡Œ =====
def process_csv_questions(csv_path, image_folder, OPENAI_API_KEY, batch_size=3):
    results = []

    # é å…ˆè¼‰å…¥æ‰€æœ‰åœ–ç‰‡æª”å
    image_files = sorted([
        os.path.join(image_folder, f)
        for f in os.listdir(image_folder)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ])

    # æ‰¹æ¬¡åŒ–åœ–ç‰‡
    def get_batches():
        for i in range(0, len(image_files), batch_size):
            yield image_files[i:i + batch_size]

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, 1):
            question = row['Question']
            reference = row['Short Answer']
            print(f"\n============= å•é¡Œ {idx}: {question} =============")

            mcp_summaries = []
            openai_summaries = []

            # æ¯é¡Œæ‰€æœ‰ batch éƒ½é MiniCPM å’Œ OpenAI
            for i, image_batch_paths in enumerate(get_batches(), 1):
                # ğŸ‘‰ MiniCPM å¤šåœ–è™•ç†
                try:
                    images = [Image.open(p).convert("RGB").resize((720, 720)) for p in image_batch_paths]
                    mcp_summary = ask(images, question)
                except Exception as e:
                    print(f"âŒ MiniCPM åœ–ç‰‡æ‰¹æ¬¡ {i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    mcp_summary = "(ERROR)"
                mcp_summaries.append(mcp_summary)

                # ğŸ‘‰ OpenAI æ‹¼åœ–å¾Œè™•ç†
                try:
                    stitched_img = stitch_images(image_batch_paths)
                    openai_summary = openai_inference(stitched_img, question, OPENAI_API_KEY)
                except Exception as e:
                    print(f"âŒ OpenAI åœ–ç‰‡æ‰¹æ¬¡ {i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    openai_summary = "(ERROR)"
                openai_summaries.append(openai_summary)

            # æ•´åˆæ‘˜è¦æå•ä¸€æ¬¡
            final_mcp = summarize_all(mcp_summaries, question)
            final_openai = summarize_all(openai_summaries, question)

            # è©•ä¼°
            mcp_metrics = evaluate_answer(final_mcp, reference)
            openai_metrics = evaluate_answer(final_openai, reference)

            print(f"ğŸŸ¢ MiniCPM å›ç­”ï¼š{final_mcp}")
            print(f"ğŸ”µ OpenAI å›ç­”ï¼š{final_openai}")
            print(f"â¡ï¸ ROUGE MiniCPM: {mcp_metrics['rougeL_f1']} | OpenAI: {openai_metrics['rougeL_f1']}")
            print(f"â¡ï¸ Hit MiniCPM: {mcp_metrics['substring_hit']} | OpenAI: {openai_metrics['substring_hit']}")

            results.append({
                "id": idx,
                "question": question,
                "reference": reference,
                "mcp_prediction": final_mcp,
                "openai_prediction": final_openai,
                **{
                    "mcp_rouge": mcp_metrics['rougeL_f1'],
                    "mcp_hit": mcp_metrics['substring_hit'],
                    "openai_rouge": openai_metrics['rougeL_f1'],
                    "openai_hit": openai_metrics['substring_hit']
                }
            })

    return results


# ===== ä¸»æµç¨‹ =====
if __name__ == '__main__':
    start_time = time.time()
    csv_path = './QA/åœ‹æ³°æ³•èªªæœƒç°¡å ±QA - è³‡æ–™æå–å•é¡Œ.csv'
    folder = './output_images_fitz'
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    results = process_csv_questions(csv_path, folder, OPENAI_API_KEY, batch_size=3)

    # çµ±è¨ˆ
    total_rouge_mcp = 0
    total_hit_mcp = 0
    total_rouge_openai = 0
    total_hit_openai = 0
    count = len(results)

    print("\nğŸ“‹ æ¯ä¸€é¡Œè©•ä¼°çµæœï¼š")
    for r in results:
        total_rouge_mcp += r['mcp_rouge']
        total_hit_mcp += r['mcp_hit']
        total_rouge_openai += r['openai_rouge']
        total_hit_openai += r['openai_hit']

    # å¹³å‡
    def safe_avg(total, count):
        return round(total / count, 4) if count > 0 else 0.0

    print("\nğŸ“Š è©•ä¼°ç¸½çµï¼š")
    print(f"ğŸŸ¢ MiniCPM - å¹³å‡ ROUGE-L F1: {safe_avg(total_rouge_mcp, count)}")
    print(f"ğŸŸ¢ MiniCPM - å¹³å‡ Substring Hit: {safe_avg(total_hit_mcp, count)}")
    print(f"ğŸ”µ OpenAI - å¹³å‡ ROUGE-L F1: {safe_avg(total_rouge_openai, count)}")
    print(f"ğŸ”µ OpenAI - å¹³å‡ Substring Hit: {safe_avg(total_hit_openai, count)}")
    
    end_time = time.time()  # çµæŸè¨ˆæ™‚
    total_time = round(end_time - start_time, 2)
    minutes, seconds = divmod(total_time, 60)
    print(f"\nâ±ï¸ ç¸½åŸ·è¡Œæ™‚é–“ï¼š{int(minutes)} åˆ† {int(seconds)} ç§’")



