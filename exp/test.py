import time
from PIL import Image
import torch
import os
import csv
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from rouge_score import rouge_scorer
from dotenv import load_dotenv
load_dotenv()
import re
import unicodedata
import openai
import base64
import io
import time
import pandas as pd

# æ¸¬è©¦åœ–ç‰‡è·¯å¾‘
image_path = "./data/pdf_img/1Q25_MP_Chinese_Vupload/page_1.png"
img = Image.open(image_path).resize((720, 720))

# å•é¡Œ
question = "é€™ä¸€é æ˜¯å¦èˆ‡ã€æ¯è‚¡ç›ˆé¤˜ã€æœ‰é—œï¼Ÿè«‹åªå›ç­”æ˜¯æˆ–å¦"

# ===== åˆå§‹åŒ–æ¨¡å‹ =====
model = AutoModel.from_pretrained(
    'openbmb/MiniCPM-V-2_6',
    trust_remote_code=True,
    attn_implementation='sdpa',
    torch_dtype=torch.bfloat16
).eval().cuda()

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)

# åŒ…è£æˆ VLM æ¨è«–å‡½å¼ï¼ˆç”¨ MiniCPMï¼‰
def vlm_answer_debug(image, question):
    full_prompt = f"{question}\n\nè«‹åªå›ç­”ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ã€‚ä¸è¦åŠ å…¶ä»–æ–‡å­—ã€‚"
    msgs = [{'role': 'user', 'content': [image, full_prompt]}]

    print("ğŸš€ ç™¼é€ VLM...")
    start = time.time()
    with torch.no_grad():
        answer = model.chat(
            image=image,  # é‡é»ï¼šé€™è£¡ä¸€å®šè¦æœ‰ image
            msgs=msgs,
            tokenizer=tokenizer,
            sampling=True,
            temperature=0.2,
            max_new_tokens=128
        )
    end = time.time()
    print(f"âœ… å›ç­”å®Œæˆï¼Œè€—æ™‚ {end - start:.2f} ç§’")
    print("ğŸ“£ æ¨¡å‹å›ç­”ï¼š", answer)
    return answer

# åŸ·è¡Œæ¸¬è©¦
vlm_answer_debug(img, question)
